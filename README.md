# DeepSparse + Milvus: Text-Search with BERT

This example demonstrates how to create a semantic search engine using FastAPI, DeepSparse, Milvus, and MySQL.

We will create 4 services:
- Milvus Server - vector database used to hold the embeddings of the dataset and perform the search queries
- MySQL Server - holds the mapping from Milvus Ids to original title,text pairs
- DeepSparse Server - inference runtime used to generate the embeddings for the queries
- Application Server - endpoint called by the client side with queries for searching

We will demonstrate running on a local machine as well as in a VPC on AWS with independent-scaling of the App, Database, and Model Serving Components.

## Text-Search with BERT - How/Why it Works

Deep Learning models like BERT are useful for building search engines. BERT takes unstructured input like text and projects the text into a fixed-length vector (called an embedding). These embeddings are generally useful for encoding the semantic meaning of the original text, with interesting relationships like the embedding of `queen` being approximately equal to the embedding of `king - man + woman`.

Because these embeddings encode semantic meaning of text, they can be used to create a search engine over a document list. First, we project each document into the embedding space with BERT. Then, when a user provides a query, we project the query into the same embedding space with BERT. Once the query and document list are both in the embedding space, we can run a nearest neighbors algorithm to find the most semantically similiar examples!

#### Seems Simple Enough: Why Do We Need DeepSparse and Milvus?

While the ideas are quite simple, there is signficiant computation in two steps. DeepSparse and Milvus make it easy for users to handle the compute demands of these key functions.

**1. Generating the embeddings:** BERT is a large model which requires compute to run, especially at large sequence lengths. This has a big impact to the system design in two steps. First, generating the embeddings for the document databases (which can have millions or more documents) is an offline batch process which can take a long time and require significiant resources (especially if embeddings are updated consistently). Second, responding to queries in an online fashion is latency-senstive. Running a big model like BERT can dominate the query response time. Without DeepSparse, users often turn to specialized hardware like GPUs to accelerate the computation, complicating the deployment process.

- DeepSparse is CPU-only deep learning deployment platform. DeepSparse can reach GPU-class performance on inference-optimized sparse models. This means that users can achieve the level of performance needed for production without the burdens of running with specialized hardware.  Users can scale DeepSparse deployments vertically from 2 to 192 cores, tailoring the size of their footprint to their applicationâ€™s specific needs. Users can scale DeepSparse horizontally with standard Kubernetes, including using managed Kubernetes services like GKE. Users can deploy the same models on any hardware from Intel to AMD to ARM and from cloud to data center to edge, including on pre-existing systems. With software-defined inference, you can simplify the model deployment process without compromising on performance, bringing more models to production and reducing overall TCO.

**2. Finding the closest vectors:** Running the naive algorithm to find the closest vectors in the embedding space is expensive. A user would have to compute the distance of the query to every element in the dataset and then run a sort algorithm `O(N*log(N))`. This is prohibative for a latency-senstive query. Without Milvus, users can only run at small scale or need to stand up complex systems yourself that can store and run the search faster. 

- Milvus with singular goal: store, index, and manage massive embedding vectors generated by deep neural networks and other machine learning (ML) models. As a database specifically designed to handle queries over input vectors, it is capable of indexing vectors on a trillion scale. Under the hood, Milvus creates [indexes](https://milvus.io/docs/v2.1.x/index.md) in a smart way and has efficient implmentations of Approximate nearest neighbor (ANN) search algorithms are used to accelerate the searching process. In such a way, Milvus makes it trivial to add vector search into an application.

## Application Architecture

We have provided a sample dataset in `client/example.csv`. These data are articles about various topics, in `(title,text)` pairs.

For each article, we project the `text` into the embedding space with sparsified BERT running on DeepSparse. We then store each vector embedding in Milvus with a unique primary key `id` and store the `(id,title,text)` tripes in MySQL. DeepSparse, Milvus, and MySQL are each independent servers with REST APIs exposed.

We have a app server built on FastAPI. The app server exposes a both a `/load` and `/search` endpoints. 

The `load` endpoint accepts a csv file with `(title, text)` representing a series of articles (the `text` field) with their shorter `title`. On `/load`, the application sends the `text` to DeepSparse Server, which returns an embedding for each `text`. These embeddings are then pushed into Milvus with a corresponding primary key `id`. The `id, title, text` tripes are then inserted into MySQL for storage.

The `search` endpoint enables clients to send `text` to the server via `GET`. The app server sends the query `text` to DeepSparse Server, which returns the embedding of the query. This embedding is sent to Milvus, which searches for the 10 most similiar vectors in the database and returns their `ids` to the app server. The app server then looks up the `title` and `text` of the IDs in MySQL and returns them back to the client.

As such, we have a microservices architecture and can scale the app server, databases, and model service independently!

## Running Locally

### Start the Server

#### Installation:
- Milvus and Postgres are installed using Docker containers. [Install Docker](https://docs.docker.com/engine/install/) and [Docker Compose](https://docs.docker.com/compose/install/linux/).
- DeepSparse is installed via PyPI. Create a virtual enviornment and run `pip install -r server/deepsparse-requirements.txt`.
- The App Server is based on FastAPI. Create a virtual enviornment and run `pip install -r server/app-requirements.txt`.

#### 1. Start Milvus

Milvus has a convient `docker-compose` file which can be downloaded with `wget` that launches the necessary services needed for Milvus. 

``` bash
cd server/database-server
wget https://raw.githubusercontent.com/milvus-io/milvus/master/deployments/docker/standalone/docker-compose.yml -O docker-compose.yml
sudo docker-compose up
cd ..

```
This command should create `milvus-etcd`, `milvus-minio`, and `milvus-standalone`.

#### 2. Start MySQL

MySQL can be started with the base MySQL image available on Docker Hub. Simply run the following command.

```bash
docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7
```

Our Databases have been started~

#### 3. Start DeepSparse Server

DeepSparse not only includes high performance runtime on CPUs, but also comes with tooling that simplify the process of adding inference to an application. Once example of this is the Server functionality, which makes it trivial to stand up a model service using DeepSparse.

We have provided a configuration file in `/server/deepsparse-server/server-config-deepsparse.yaml`, which sets up an embedding extraction endpoint running a sparse version of BERT from SparseZoo. You can edit this file to adjust the number of workers you want (this is the number of concurrent inferences that can occur). Generally, its a fine starting point to use `num_cores/2`.

Here's what the config file looks like.

```yaml
num_workers: 4  # number of streams - should be tuned, num_cores / 2 is good place to start

endpoints: 
  - task: embedding_extraction
    model: zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni
    route: /predict
    name: embedding_extraction_pipeline
    kwargs:
      return_numpy: False
      extraction_strategy: reduce_mean
      sequence_length: 512
      engine_type: deepsparse
```

To start DeepSparse, run the following:

```bash
deepsparse.server --config_file server/deepsparse-server/server-config-deepsparse.yaml
```

TO BE REMOVED --- hack to remove bug in Server

- Run `vim deepsparse-env/lib/python3.8/site-packages/deepsparse/server/server.py`
- In `_add_pipeline_endpoint()`, udpate `app.add_api_route` by commenting out `response_model=output_schema`.

ESC-I enters insert mode; ESC Exits insert mode. :wq writes file and quits.

**Potential Improvements**

With this search application, there is both a throughput-focused step (`load`) where we need to process a large number of embeddings at once with no latency requirements and there is a latency-focused step (`search`) where we need to process one embedding quickly and return to the user as fast as possible. For simplicity, we currently only use one configuration of DeepSparse with batch size 1, which is more of a latency oriented setup.

An extension to this project would be configuring DeepSparse to have multiple endpoints or adding another DeepSparse Server instance with a configuration for high throughput.

#### 4. Start The App Server

The App Server is built on `FastAPI` and `uvicorn` and orchestrates DeepSparse, Milvus, and MySQL to create a search engine. 

Run the following to launch.

```bash
python3 server/app-server/src/app.py
```

### Use the Search Engine!

We have provided both a latency testing script and a Jupyter notebook to interact with the server. 

The Jupyter notebook is self-documenting and is a good starting point to play around with the application.

You can run with the following command:
`juptyer notebook example-client.ipynb`

The latency testing script generates multiple clients to test response time from the server. It provides metrics on both overall query latency as well as metrics on the model serving query latency (the end to end time from the app server querying DeepSparse until a response is returned.) 

You can run with the following command:
```bash
python3 client/latency-test-client.py --url http://localhost:5000/ --dataset_path client/example.csv --num_clients 8
```
- `--url` is the location of the app server
- `--dataset_path` is the location of the dataset path on client side
- `--num_clients` is the number of clients that will be created to send requests concurrently

## Running in an AWS VPC with Independent-Scaling

### Create a VPC

First, we will create a VPC that houses our instances and enables us to communicate between the App Server, Milvus, MySQL, and DeepSparse.

- Navigate to `Create VPC` in the AWS console
- Select `VPC and more`. Name it `semantic-search-demo-vpc`
- Make sure you have `IPv4 CIDR block` set. We use `10.0.0.0/16` in the example.
- Number of AZs to 1, Number of Public Subnets to 1, and Number of Private Subnets to 0.

When we create our services, we will add them to the VPC and only enable communication to the backend model service and databases from within the VPC, isloating the model and database services from the internet.

### Create the Database Server

Launch an EC2 Instance.
- Navigate to EC2 > Instances > Launch an Instance
- Name the instance `database-server`
- Select Amazon Linux and a `t2.micro` instance type

Edit the `Network Setting` to expose the App Endpoint to the Internet while still giving access to the backend database and model service.
- Put the `app-server` into the `semantic-search-demo-vpc` VPC
- Choose the public subnet
- Set `Auto-Assign Public IP` to `Enabled`.
- Select Create Security Group
- Leave the SSH security group rule in place
- Add a `Custom TCP` security group rule with port `19530` with `source-type` of `Custom` and Source equal to the CIDR of the VPC (in our case `10.0.0.0/16`). This is how the App Server will Talk to Milvus
- Add a `Custom TCP` security group rule with port `3306` with `source-type` of `Custom` and Source equal to the CIDR of the VPC (in our case `10.0.0.0/16`). This is how the App Server will Talk to MySQL

Click Launch Instance.

Next, SSH into your newly created instance and launch the app server.

From the command line run:
```
ssh -i path/to/your/private-key.pem ec2-user@your-instance-public-ip
```

We will use Docker to launch the applicaiton.

Install Docker and add group membership for the default ec2-user so you can run all docker commands without using the sudo command:
```
sudo yum update -y
sudo yum install docker -y
sudo usermod -a -G docker ec2-user
id ec2-user
newgrp docker
``

Start Docker and Check it is running with the following:
```
sudo service docker start
docker container ls
```

Install `docker-compose` with: 
```
pip3 install --user docker-compose
```

Download Milvus Docker Image and Launch Milvus with `docker-compose`:
```
wget https://raw.githubusercontent.com/milvus-io/milvus/master/deployments/docker/standalone/docker-compose.yml -O docker-compose.yml
docker-compose up
```

SSH from another terminal into the same instance to setup MySQL.
```
ssh -i path/to/your/private-key.pem ec2-user@your-instance-public-ip
```

Docker should already be installed, so run the following to launch MySQL.

```bash
docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7
```
Running `docker container ls` should show 4 containers running (`mysql`, `milvus-standalone`, `milvus-etcd`, and `milvus-minio`).

Your databases are up and running!

### Create the Application Server

Launch an EC2 Instance.
- Navigate to EC2 > Instances > Launch an Instance
- Name the instance `app-server`
- Select Amazon Linux and a `t2.micro` instance type

Edit the `Network Setting` to expose the App Endpoint to the Internet while still giving access to the backend database and model service.
- Put the `app-server` into the `semantic-search-demo-vpc` VPC
- Choose the public subnet
- Set `Auto-Assign Public IP` to `Enabled`.
- Select Create Security Group
- In addition to SSH, add a `Custom TCP` security group rule with port `5000` with `source-type` of `Anywhere`. This exposes the app over HTTP.

Click Launch Instance.

Next, SSH into your newly created instance and launch the app server.

From the command line run:
```
ssh -i path/to/your/private-key.pem ec2-user@your-instance-public-ip
```

Clone this repo with Git:
```bash
sudo yum update -y
sudo yum install git -y
sudo git clone https://github.com/rsnm2/deepsparse-milvus.git
```

TO BE REMOVED --- hack to remove bug in Server

- Run `vim deepsparse-env/lib/python3.7/site-packages/deepsparse/server/server.py`
- In `_add_pipeline_endpoint()`, udpate `app.add_api_route` by commenting out `response_model=output_schema`.

ESC-I enters insert mode; ESC Exits insert mode. :wq writes file and quits.

Install App Requirements in a virutal enviornment.
```bash
python3 -m venv app-env
source app-env/bin/activate
pip3 install -r deepsparse-milvus/text-search-engine/server/app-requirements.txt
```

Run the following to activate.
```bash
python3 deepsparse-milvus/text-search-engine/server/app-server/src/app.py
```

You should see a Uvicorn server running!

### Create DeepSparse AWS Instance

The last step is setting up a model service. Since the processing of embeddings is the most
compute heavy portion of the application, we will use an instance with more cores.

The `c6i.4xlarge` instance type. This is an 8 core instance with 32GB of RAM. Using a more powerful instance like 
this will allow us to demonstrate the ability of the DeepSparse to handle multiple concurrent requests efficiently.

Launch an EC2 Instance.
- Navigate to EC2 > Instances > Launch an Instance
- Name the instance `database-server`
- Select Amazon Linux and a `c6i.4xlarge` instance type

Edit the `Network Setting` to expose the App Endpoint to the Internet while still giving access to the backend database and model service.
- Put the `app-server` into the `semantic-search-demo-vpc` VPC
- Choose the public subnet
- Set `Auto-Assign Public IP` to `Enabled`.
- Select Create Security Group
- Leave the SSH security group rule in place
- Add a `Custom TCP` security group rule with port `5543` with `source-type` of `Custom` and Source equal to the CIDR of the VPC (in our case `10.0.0.0/16`). This is how the App Server will Talk to DeepSparse

Click Launch Instance.

Next, SSH into your newly created instance and launch the DeepSparse Server.

From the command line run:
```
ssh -i path/to/your/private-key.pem ec2-user@your-instance-public-ip
```

Clone this repo with Git:
```bash
sudo yum update -y
sudo yum install git -y
git clone https://github.com/rsnm2/deepsparse-milvus.git
```

Install App Requirements in a virutal enviornment.
```bash
python3 -m venv deepsparse-env
source deepsparse-env/bin/activate
pip3 install -r deepsparse-milvus/text-search-engine/server/deepsparse-requirements.txt
```

Run the following to start a model server with DeepSparse as the runtime engine. 
```bash
deepsparse.server --config-file deepsparse-milvus/text-search-engine/server/deepsparse-server/server-config-onnxruntime.yaml```
```

You should see a Uvicorn server running!

We have also provided a config file with ONNX as the runtime engine for performance comparison. You can launch a server with ONNX Runtime with the following:
```bash
deepsparse.server --config-file deepsparse-milvus/text-search-engine/server/deepsparse-server/server-config-onnx.yaml
```
**Note: you should have either DeepSparse or ONNXRuntime running but not both***

## **Test Performance**

From your local machine, run the following, which creats 8 clients that continously make requests to the server.

```bash
python3 client/latency-test-client.py --url http://app-server-public-ip:5000/ --dataset_path client/example.csv --num_clients 8 --iters_per_client 25
```

With DeepSparse running in the Model Server, the latency looks like this, where Model Latency is the time it takes to process
a request by Model Server and Query Latency is the full end to end time on the client side (Network Latency + Model Latency + Database Latency).

```
Model Latency Stats:
{'count': 100,
 'mean': 101.46937451000213,
 'median': 101.42159349993563,
 'std': 0.8363166606434379}

Query Latency Stats:
{'count': 100,
 'mean': 290.0473149998288,
 'median': 233.32067500450648,
 'std': 224.45764981095994}
```

With ONNX Runtime running in the Model Server, the latency looks like this:

```
Model Latency Stats:
{'count': 100,
 'mean': 101.46937451000213,
 'median': 101.42159349993563,
 'std': 0.8363166606434379}

Query Latency Stats:
{'count': 100,
 'mean': 290.0473149998288,
 'median': 233.32067500450648,
 'std': 224.45764981095994}
```

This has created an **xxxx** speedup in Model Latency.
